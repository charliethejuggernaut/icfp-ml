{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c214a932",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Below we give a simple piece of code in order to compute a PCA in 2 dimensions for digits (using the scikit-learn library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f369d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as plt_cm\n",
    "import matplotlib.colors as plt_col\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(x, classes, ax=None):\n",
    "    xx = x[0,:]\n",
    "    yy = x[1,:]\n",
    "    width = np.max(xx) - np.min(xx)\n",
    "    height = np.max(yy) - np.min(yy)\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    ax.set_xlim([np.min(xx) - 0.1 * width, np.max(xx) + 0.1 * width])\n",
    "    ax.set_ylim([np.min(yy) - 0.1 * height, np.max(yy) + 0.1 * height])\n",
    "    cmap = plt_cm.jet\n",
    "    norm = plt_col.Normalize(vmin=0, vmax=9)\n",
    "    mapper = plt_cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    colors = mapper.to_rgba(range(10))\n",
    "    for x1, x2, digit in zip(xx, yy, classes):\n",
    "        ax.text(x1, x2, digit, color=colors[int(digit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f67774",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "digits = ds.load_digits()\n",
    "digits_i = np.random.choice(range(digits.data.shape[0]), 500)\n",
    "digits_x = np.transpose(digits.data[digits_i, :])\n",
    "digits_classes = digits.target[digits_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, figsize=[15, 6])\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    ax[i].matshow(digits.images[i], cmap=plt_cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b379331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_r = pca.fit(digits_x.T).transform(digits_x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a98f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(x_r.T,digits_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31714f3d",
   "metadata": {},
   "source": [
    "# Probabilisitic PCA\n",
    "\n",
    "\n",
    "Probabilsitic PCA is simple linear-Gaussian framework defined by a matrix $W\\in \\mathbb{R}^{d\\times q}$, a vector $\\mu\\in \\mathbb{R}^d$ and a noise parameter $\\sigma^2>0$ such that the observed variable $x$ is modeled as:\n",
    "\\begin{eqnarray*}\n",
    "{x} = W{z} + {\\mu}+{\\epsilon}\n",
    "\\end{eqnarray*}\n",
    "where the $q$-dimensional latent variable $z$ and the $d$-dimensional noise $\\epsilon$ are independent Gaussian random variables: $p(z,\\epsilon)=p(z)p(\\epsilon) =\\mathcal{N}(z|0,I_q)\\mathcal{N}(\\epsilon |0,\\sigma^2I_d)$.\n",
    "\n",
    "We can equivalently write: $p(x|z) =\\mathcal{N}(x|Wz+\\mu, \\sigma^2I_d)$.\n",
    "\n",
    "Note that $x,\\mu,\\epsilon\\in \\mathbb{R}^d$ and $z\\in \\mathbb{R}^q$ with typically $q<< d$.\n",
    "\n",
    "### Question 1 (Maths)\n",
    "\n",
    "Show that $p(x) = \\mathcal{N}(x|\\mu,C)$ with $C=WW^T+\\sigma^2 I_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b935ab1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab85bffe",
   "metadata": {},
   "source": [
    "### Question 2 (Maths)\n",
    "\n",
    "We define $M = W^TW + \\sigma^2 I_q\\in \\mathbb{R}^{q\\times q}$. Show that\n",
    "\\begin{eqnarray*}\n",
    "p(z|x) = \\mathcal{N}(z|M^{-1}W^T(x-\\mu), \\sigma^2M^{-1}).\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14cd64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f96adf53",
   "metadata": {},
   "source": [
    "# Maximum likelihood PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10a505b",
   "metadata": {},
   "source": [
    "### Question 3 (Maths)\n",
    "\n",
    "Show that the log likelhood function is given by\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L} \\stackrel{\\Delta}{=} \\sum_{n=1}^N \\ln p(x_n |W,\\mu, \\sigma^2) = -\\frac{N}{2}\\left( d\\ln(2\\pi) +\\ln |C| +\\text{Tr}(C^{-1}S)  \\right)\n",
    "\\end{eqnarray*}\n",
    "with $S = \\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu)(x_n-\\mu)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a659d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77f0d50",
   "metadata": {},
   "source": [
    "### Question 4 (Maths)\n",
    "\n",
    "We have $\\nabla_\\mu \\mathcal{L} = \\sum_n(x_n-\\mu)C^{-1}$ so that $\\nabla_\\mu \\mathcal{L} =0$ iff $\\mu =\\frac{1}{n} \\sum_n x_n$ as expected. The optimization in $W$ is a bit more challenging. First, we have $\\nabla_W \\mathcal{L} = -N\\left( C^{-1}W -C^{-1}SC^{-1}W \\right)$ so that $\\nabla_W \\mathcal{L} = 0$ iff $W=SC^{-1}W$.\n",
    "\n",
    "We assume that $W\\neq0$ and $C\\neq S$.\n",
    "\n",
    "We denote the SVD of W as $W=ULV^T$ with $U\\in \\mathbb{R}^{d\\times q}$, $V\\in \\mathbb{R}^{q\\times q}$ and $L$ the diagonal matrix of the singular values. Show that $W=SC^{-1}W$ implies $SUL = U (\\sigma^2 I + L^2) L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b456285",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b810e5df",
   "metadata": {},
   "source": [
    "Hence if $U=(u_1,\\dots, u_q)$ with $u_i\\in \\mathbb{R}^{d}$, and $L =\\text{diag}(\\ell_1, \\dots ,\\ell_q)$,  we have $\\ell_i Su_i = \\ell_i(\\sigma^2+\\ell_i^2)u_i$ and if $\\ell_i\\neq 0$, then $u_i$ is an eigenvector of $S\\in \\mathbb{R}^{d \\times d}$ with associated eigenvalue $\\lambda_i = \\sigma^2+\\ell_i^2$. \n",
    "\n",
    "The covariance matrix $S \\in \\mathbb{R}^{d\\times d}$ is a positive semi-definite symmetric matrix with eigenvactors $u^S_i$ and associated eigenvalues $\\lambda_i\\geq 0$ for $i=1,\\dots, d$. We assume that $\\lambda_1\\geq \\lambda_2\\geq \\dots\\lambda_q\\geq 0$. \n",
    "\n",
    "Show that all potential solutions for $W$ can be written $W=U^S_q(K_q-\\sigma^2 I_q)^{1/2}R$ where $U^S_q$ is a $d\\times q$ matrix comprising $q$ eigenvectors of $S$ and $K_q$ is a diagonal matrix and $R$ is an arbitrary orthogonal matrix. Explicit the possible values for the diagonal terms of $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fcf0f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222bb239",
   "metadata": {},
   "source": [
    "### Question 5 (Maths)\n",
    "\n",
    "It can be shown (but this is not asked!) that the optimun $W$ maximizing the likelihood correspond to a case where $U^S_q =(u^S_1,\\dots u^S_q)$ i.e. contains the $q$ eigenvectors of $S$ associated to the largest eigenvalues and $K_q=\\text{diag}(\\lambda_1,\\dots \\lambda_q)$. The optimum $\\sigma^2$ is $\\sigma^2=\\frac{1}{d-q}\\sum_{i=q+1}^d\\lambda_i$ the average of the 'discarded' eignevalues.\n",
    "\n",
    "Show that the law of $x$ does not depend on $R$. This invariance can be understood in terms of rotations within the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0d83f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "696eb8aa",
   "metadata": {},
   "source": [
    "### Question 6 (Code)\n",
    "\n",
    "Complete the code below to compute the parameters $\\mu= =\\frac{1}{n} \\sum_n x_n$, $W = U^S_q(K_q-\\sigma^2 I_q)^{1/2}$ and $\\sigma^2=\\frac{1}{d-q}\\sum_{i=q+1}^d\\lambda_i$ in the `fit_ml` method and then compute the mean of the latent variable $z$ knowing $x$ as $M^{-1}W^T(x-\\mu)$.\n",
    "\n",
    "Hint: you can use `np.linalg.svd` `np.eye` and `np.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5964e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPCA():\n",
    "    def __init__(self, q=2):\n",
    "        self.q = q\n",
    "        self.w = None\n",
    "        self.mu = None\n",
    "        self.sigma2 = None\n",
    "        \n",
    "    def fit_ml(self, x):\n",
    "        self.x = x\n",
    "        (d,n) = x.shape\n",
    "        # your code here\n",
    "        pass\n",
    "    \n",
    "    def transform(self):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3879935",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca = PPCA()\n",
    "ppca.fit_ml(digits_x)\n",
    "z_ml = ppca.transform() # shape of z should be (2,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da66067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(z_ml,digits_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db8d91",
   "metadata": {},
   "source": [
    "# EM algorithm for PCA\n",
    "\n",
    "We now derive an altermative EM algorithm for the probabilistic PCA.\n",
    "We start by introducing the complete log-likelihood (by incorporating the latent variable):\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L}_C = \\sum_{n=1}^N \\ln p(x_n,z_n)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We introduce the notation $\\langle \\cdot \\rangle$ as the expectation with respect to $p(z|x)$. So that we have\n",
    "\\begin{eqnarray*}\n",
    "\\langle z_n \\rangle &=& M^{-1}W^T(x_n-\\mu)\\\\\n",
    "\\langle z_n z_n^T \\rangle - \\langle z_n \\rangle \\langle z_n \\rangle^T &=& \\text{cov}[z_n]\\\\\n",
    "&=& \\sigma^2 M^{-1}\n",
    "\\end{eqnarray*}\n",
    "by the result of question 2 above.\n",
    "\n",
    "We then define the expected log-likelihood $\\langle \\mathcal{L}_C \\rangle$.\n",
    "\n",
    "### Question 7 (Maths)\n",
    "\n",
    "Show that\n",
    "\\begin{eqnarray}\n",
    "\\label{eq:Lc}\\langle \\mathcal{L}_C \\rangle = \\text{cst} -\\sum_{n=1}^N \\frac{d}{2}\\ln\\sigma^2 +\\frac{\\|x_n-\\mu\\|^2}{2\\sigma^2}+\\frac{1}{2}\\text{tr}\\langle z_nz_n^T \\rangle + \\frac{\\text{tr}(W^TW\\langle z_n z_n^T\\rangle)}{2\\sigma^2} - \\frac{\\langle z_n \\rangle^TW^T(x_n-\\mu)}{\\sigma^2},\n",
    "\\end{eqnarray}\n",
    "where cst does not depend on $\\mu, W$ or $\\sigma^2$.\n",
    "\n",
    "Hint: check that $\\langle \\|z\\|^2\\rangle = \\text{tr}(\\langle z_nz_n^T \\rangle)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257e5b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c6e9335",
   "metadata": {},
   "source": [
    "We now derive the EM algorithm to estimate the parameters $W$ and $\\sigma^2$. Indeed, we know that our best estimate for $\\mu$ is $\\overline{x} = \\frac{1}{n}\\sum x_n$, so that we set $\\mu = \\overline{x}$.\n",
    "\n",
    "For the E-step, given our current parameters $W$ and $\\sigma^2$, we compute:\n",
    "\\begin{eqnarray*}\n",
    "\\langle z_n \\rangle &=& M^{-1}W^T(x_n-\\mu)\\\\\n",
    "\\langle z_n z_n^T \\rangle = \\sigma^2 M^{-1} +\\langle z_n \\rangle \\langle z_n \\rangle^T,\n",
    "\\end{eqnarray*}\n",
    "where $M=W^TW+ \\sigma^2 I$.\n",
    "\n",
    "For the M-step, we maximize $\\langle \\mathcal{L}_C \\rangle$ given in question 7 with respect to $W$ and $\\sigma^2$, so that we get (differentiating and setting the derivative to zero):\n",
    "\\begin{eqnarray*}\n",
    "W_{\\text{new}} &=& \\left( \\sum_n (x_n-\\mu) \\langle z_n \\rangle ^T\\right)\\left( \\sum_n \\langle z_n z_n^T \\rangle \\right)^{-1}\\\\\n",
    "\\sigma^2_{\\text{new}} &=& \\frac{1}{Nd}\\sum_n\\left( \\|x_n-\\mu\\|^2 - 2\\langle z_n \\rangle ^T W_{\\text{new}}^T(x_n-\\mu) + \\text{tr}(\\langle z_n z_n^T \\rangle W_{\\text{new}}^TW_{\\text{new}})\\right)\n",
    "\\end{eqnarray*}\n",
    "(Not asked but if you want to check this formula you can have a look at [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf))\n",
    "\n",
    "### Question 8 (Code)\n",
    "\n",
    "Complete the code below to compute the parameters with the EM algorithm. The `transform` method should be the same as above.\n",
    "\n",
    "Hint: I find [`np.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ced76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPCA():\n",
    "    def __init__(self, d=64, q=2, sigma2 = 0.1):\n",
    "        self.q = q\n",
    "        self.w = np.random.rand(d, q)\n",
    "        self.sigma2 = sigma2\n",
    "        self.mu= None\n",
    "    \n",
    "    def fit_em(self, x, max_iter = 25):\n",
    "        self.x = x\n",
    "        (d,n) = x.shape\n",
    "        # your code here\n",
    "        pass\n",
    "    \n",
    "    def transform(self):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca = PPCA()\n",
    "ppca.fit_em(digits_x)\n",
    "z_em = ppca.transform() # shape of z should be (2,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(z_em,digits_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b08d4c",
   "metadata": {},
   "source": [
    "### Question 9 (Maths)\n",
    "\n",
    "Explain the result of the code given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41314675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this code\n",
    "[u, s, v] = np.linalg.svd(z_em @ z_ml.T)\n",
    "r = v @ u.T\n",
    "z_em_r = r @ z_em\n",
    "plt.scatter(z_em_r[0,:], z_em_r[1,:])\n",
    "plt.scatter(z_ml[0,:], z_ml[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9794081",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-icfp",
   "language": "python",
   "name": "ml-icfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
