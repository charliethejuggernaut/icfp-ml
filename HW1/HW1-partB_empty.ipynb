{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4524aa52",
   "metadata": {},
   "source": [
    "# Homework 1 - Part B\n",
    "\n",
    "Your name:\n",
    "\n",
    "Email (one used on the moodle): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb08ed",
   "metadata": {},
   "source": [
    "## Naive Bayes and logistic regression for Gaussian mixtures\n",
    "\n",
    "In this exercise, we will explorre the relation between the generative classifier: Naive Bayes; and the discriminative classifier: logistic regression. We did study these classifiers empirically in [Naive Bayes and logistic regression](https://github.com/mlelarge/icfp-ml/blob/main/solutions/01_NaivesBayes_Logistic.ipynb)\n",
    "\n",
    "Instead of using a dataset as we did in the practical above, we will use samples from a mixture of Gaussians in $2$ dimension. You should not modify the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72780d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "p = 0.5\n",
    "m0 = [-1.5,-1.5]\n",
    "m1 = [1.5,1.5]\n",
    "s1 = 1.\n",
    "s2 = 4\n",
    "multi_norm0 = MultivariateNormal(loc=torch.tensor(m0), covariance_matrix=torch.diag(torch.tensor([s1,s2])))\n",
    "multi_norm1 = MultivariateNormal(loc=torch.tensor(m1), covariance_matrix=torch.diag(torch.tensor([s1,s2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used for contour plot\n",
    "dtype = torch.float32\n",
    "def get_meshgrid(x0_range, x1_range, num_points=100):\n",
    "    x0 = np.linspace(x0_range[0], x0_range[1], num_points)\n",
    "    x1 = np.linspace(x1_range[0], x1_range[1], num_points)\n",
    "    return np.meshgrid(x0, x1)\n",
    "\n",
    "def contour_plot(x0_range, x1_range, logprob_fn, batch_shape, colours, levels=None, num_points=100, dtype=dtype):\n",
    "    X0, X1 = get_meshgrid(x0_range, x1_range, num_points=num_points)\n",
    "    Z = torch.exp(logprob_fn(torch.tensor(np.expand_dims(np.array([X0.ravel(), X1.ravel()]).T, 1), dtype=dtype)))\n",
    "    Z = np.array(Z.detach()).T.reshape(batch_shape, *X0.shape)\n",
    "    for batch in np.arange(batch_shape):\n",
    "        if levels:\n",
    "            plt.contourf(X0, X1, Z[batch], alpha=0.2, colors=colours, levels=levels)\n",
    "        else:\n",
    "            plt.contour(X0, X1, Z[batch], colors=colours[batch], alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "samples0 = multi_norm0.sample((n_samples,))\n",
    "samples1 = multi_norm1.sample((n_samples,))\n",
    "x_train = torch.cat((samples0,samples1)).numpy()\n",
    "y_train = torch.cat((torch.zeros(n_samples), torch.ones(n_samples))).numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 1000\n",
    "samples0_test = multi_norm0.sample((n_test,))\n",
    "samples1_test = multi_norm1.sample((n_test,))\n",
    "x_test = torch.cat((samples0_test,samples1_test)).numpy()\n",
    "y_test = torch.cat((torch.zeros(n_test), torch.ones(n_test))).numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 'Class 0', 1: 'Class 1'}\n",
    "label_colours = ['blue', 'green']\n",
    "\n",
    "def plot_data(x, y, labels, colours):\n",
    "    for c in np.unique(y):\n",
    "        inx = np.where(y == c)\n",
    "        plt.scatter(x[inx, 0], x[inx, 1], label=labels[c], c=colours[c])\n",
    "    plt.title(\"Training set\")\n",
    "    plt.xlabel(\"Feature 1: $X_1$\")\n",
    "    plt.ylabel(\"Feature 2: $X_2$\")\n",
    "    plt.legend()\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), multi_norm0.log_prob, 1, label_colours[0])\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), multi_norm1.log_prob, 1, label_colours[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06ef58",
   "metadata": {},
   "source": [
    "To formalize the current setting: we have two classes $Y\\in \\{0,1\\}$ and two features $X_1$ and $X_2$ such that:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(Y=0) = 1-\\mathbb{P}(Y=1) = p\n",
    "\\end{aligned}\n",
    "$$\n",
    "and,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(X_i=x_i | Y=c) &= N(x_i | \\mu_{ic}, \\sigma_{i})\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma_{i}^2}} \\exp\\left\\{-\\frac{1}{2} \\left(\\frac{x_i - \\mu_{ic}}{\\sigma_{i}}\\right)^2\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(X_1=x_1, X_2=x_2 | Y =c) = \\mathbb{P}(X_1=x_1 | Y=c) \\mathbb{P}(X_2=x_2 | Y=c)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note that we consider a case where the variance of the features do not depend on the class which is different from what we did during the practical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985efc06",
   "metadata": {},
   "source": [
    "## Question 1: code the Naive Bayes classifier\n",
    "\n",
    "Here you should adapt the code used in the part [Naive Bayes classifier](https://github.com/mlelarge/icfp-ml/blob/main/solutions/01_NaivesBayes_Logistic.ipynb) in order to estimate the parameters $p, \\mu_{10}, \\mu_{11}, \\sigma_1, \\mu_{20}, \\mu_{21}, \\sigma_2$.\n",
    "\n",
    "Hint: there is only a small modification to make in one of the functions from the practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def get_prior(y):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = get_prior(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal, Independent\n",
    "\n",
    "def get_class_conditionals(x, y):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b963025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_conditionals = get_class_conditionals(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caab211",
   "metadata": {},
   "source": [
    "If you used the same names as in the practical, the code below should work without any modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), class_conditionals.log_prob, 2, label_colours)\n",
    "plt.title(\"Training set with class-conditional density contours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2821bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(prior, class_conditionals, x):\n",
    "    \"\"\"\n",
    "    This function takes the prior distribution, class-conditional distribution, and \n",
    "    a batch of inputs in a numpy array of shape (batch_shape, 2).\n",
    "    This function should compute the class probabilities for each input in the batch, using\n",
    "    the prior and class-conditional distributions, according to the above equation.\n",
    "    \"\"\"\n",
    "    class_probs = class_conditionals.log_prob(x[:, None])\n",
    "    joint_likelihood = class_probs + torch.log(prior.probs).unsqueeze(0)\n",
    "    norm_factor = torch.logsumexp(joint_likelihood, dim=-1, keepdims=True)\n",
    "    log_prob = joint_likelihood - norm_factor\n",
    "    y = torch.argmax(torch.exp(log_prob), dim=-1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f238456",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_class(prior, class_conditionals, torch.tensor(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20074c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((2*x0_min, 2*x0_max), (2*x1_min, 2*x1_max), \n",
    "             lambda x: torch.log(predict_class(prior, class_conditionals, x)), \n",
    "             1, label_colours, levels=[-0.5, 0.5, 1.5, 2.5],\n",
    "             num_points=500)\n",
    "plt.title(\"Training set with decision regions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ea38e",
   "metadata": {},
   "source": [
    "## Question 2: plot the ROC curve corresponding to Naive Bayes\n",
    "\n",
    "Hint: use [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) from the scikit-learn library. For this, you will need to first compute the likelihood from your Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94020e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the plot for Naive Bayes\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb8385",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "We now consider another classifier with the logistic regression.\n",
    "Here we can use directly the code from the practical as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac944a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)\n",
    "\n",
    "epochs = 4000\n",
    "input_dim = 2 # Two inputs x1 and x2 \n",
    "output_dim = 2 \n",
    "learning_rate = 0.005\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer,x_train,y_train,x_test,y_test,n_epochs=epochs,freq_print=200):\n",
    "    losses = []\n",
    "    losses_test = []\n",
    "    Iterations = []\n",
    "    total_test = y_test.size(0)\n",
    "    total = y_train.size(0)\n",
    "    iter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        outputs = model(x_train)\n",
    "        \n",
    "        loss = criterion(outputs.squeeze(), y_train) \n",
    "        optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "        loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "        optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "        iter+=1\n",
    "        if iter%freq_print==0:\n",
    "            # calculate Accuracy\n",
    "            with torch.no_grad():\n",
    "                # Calculating the loss and accuracy for the test dataset\n",
    "                outputs_test = torch.squeeze(model(x_test))\n",
    "                loss_test = criterion(outputs_test, y_test)\n",
    "\n",
    "                _,predicted_test = torch.max(outputs_test.data,1)\n",
    "                correct_test = torch.sum(predicted_test == y_test.data)\n",
    "                accuracy_test = 100 * correct_test/total_test\n",
    "                losses_test.append(loss_test.item())\n",
    "                \n",
    "                # Calculating the loss and accuracy for the train dataset\n",
    "                _,preds = torch.max(outputs.data,1)\n",
    "                correct = torch.sum(preds == y_train.data)\n",
    "                accuracy = 100 * correct/total\n",
    "                losses.append(loss.item())\n",
    "                Iterations.append(iter)\n",
    "\n",
    "                print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "                print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")\n",
    "    return losses, losses_test, Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ad434",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "x_trt = torch.tensor(x_train, dtype=dtype)\n",
    "x_tst = torch.tensor(x_test, dtype=dtype)\n",
    "y_trt = torch.tensor(y_train, dtype=torch.long)\n",
    "y_tst = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "losses, losses_test, Iterations = train(model,criterion,optimizer,x_trt,y_trt,x_tst,y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "model.eval()\n",
    "contour_plot((2*x0_min, 2*x0_max), (2*x1_min, 2*x1_max), \n",
    "             lambda x: torch.log(torch.argmax(model(x),-1)), \n",
    "             1, label_colours, levels=[-0.5, 0.5, 1.5, 2.5],\n",
    "             num_points=500)\n",
    "plt.title(\"Training set with decision regions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b8dd5",
   "metadata": {},
   "source": [
    "## Question 3: plot the ROC curve corresponding to logistic regression\n",
    "\n",
    "Normaly, you should be able to use the same function as the one used for the Naive Bayes ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff32e7",
   "metadata": {},
   "source": [
    "Plot the ROC curves for both classifiers on the same plot, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add both Naive Bayes and the logistic regression\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bcfa4",
   "metadata": {},
   "source": [
    "## Question 4: computing the optimal ROC curve (Maths)\n",
    "\n",
    "For this simple Gaussian mixture model, we can compute the optimal ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76245313",
   "metadata": {},
   "source": [
    "Compute the loglikelihood ratio for $x=(x_1,x_2)\\in \\mathbb{R}^2$:\n",
    "\\begin{eqnarray*}\n",
    "\\ln\\mathcal{L}(x) &=& \\ln\\frac{p(x|1)}{p(x|0)}\n",
    "\\end{eqnarray*}\n",
    "and show that there exists $c,d_1,d_2\\in \\mathbb{R}$ such that $\\ln\\mathcal{L}(x) = c+d_1x_1+d_2x_2$.\n",
    "\n",
    "Conclude that the optimal ROC curve is exactly the same ROC curve as the one computed for a mixture of $1$-dimensional Gaussian mixtures (studied in [Exact_ROC_GM](https://github.com/mlelarge/icfp-ml/blob/main/solutions/Exact_ROC_GM_sol.ipynb)) but for different values of the parameters. Give the expressions for these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6dc97",
   "metadata": {},
   "source": [
    "## Question 5: plot the optimal ROC curve\n",
    "\n",
    "Compute this ROC curve (feel free to use code from the practical) and superpose it with the ROC curves of the classifiers above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your 3 ROC curves\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot(np.linspace(0, 1, 1000), np.linspace(0, 1, 1000), linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c5ef2",
   "metadata": {},
   "source": [
    "## Question 6: Why all ROC curves agree? (Maths)\n",
    "\n",
    "Give a short explanation of why the ROC curve of Naive Bayes agree with the optimla ROC curve.\n",
    "\n",
    "Logistic regression assumes a [parametric form](https://dataflowr.github.io/slides/module3.html#16) for the distribution $p(y=1|x)=\\sigma(b+w_1x_1+w_2x_2)$, where $\\sigma(x)=1\\frac{1}{1+e^{-x}}$ is the sigmoid function. The paramerters $b,w_1,w_2$ are computed through gradient descent to maximize the log-likelihood.\n",
    "The fact that the ROC curves agree suggests that the parametric form of the logistic regression is correct for our Gaussian mixture model. Show that this is indeed the case by giving the values of $b, w_1$ and $w_2$ as a function of $p,\\mu_{10}, \\mu_{11}, \\sigma_1,\\mu_{20}, \\mu_{21}, \\sigma_2$.\n",
    "\n",
    "Hint: \n",
    "$$\n",
    "p(y=1|x)=\\frac{1}{1+\\frac{p(y=0)p(x|y=0)}{p(y=1)p(x|y=1)}}\n",
    "$$\n",
    "\n",
    "Conclude."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-icfp",
   "language": "python",
   "name": "ml-icfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
